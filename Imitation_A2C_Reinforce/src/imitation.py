import sys
import argparse
import numpy as np
import keras
import random
import gym
from keras.utils import to_categorical
import pickle
import pdb
import os


class Imitation():
    def __init__(self, model_config_path, expert_weights_path):
        # Load the expert model.
        with open(model_config_path, 'r') as f:
            self.expert = keras.models.model_from_json(f.read())
        self.expert.load_weights(expert_weights_path)
        
        # Initialize the cloned model (to be trained).
        with open(model_config_path, 'r') as f:
            self.model = keras.models.model_from_json(f.read())
        self.model.compile(optimizer='adam',
              loss='categorical_crossentropy',
              metrics=['accuracy'])

        # TODO: Define any training operations and optimizers here, initialize
        #       your variables, or alternatively compile your model here.

    def run_expert(self, env, render=False):
        # Generates an episode by running the expert policy on the given env.
        return Imitation.generate_episode(self.expert, env, render)

    def run_model(self, env, render=False):
        # Generates an episode by running the cloned policy on the given env.
        return Imitation.generate_episode(self.model, env, render)

    def _save_model_weights(self, model_weights_path):
        self.model.save_weights(model_weights_path)

    def load_model_weights(self, model_weights_path):
        self.model.load_weights(model_weights_path)

    @staticmethod
    def generate_episode(model, env, render=False):
        # Generates an episode by running the given model on the given env.
        # Returns:
        # - a list of states, indexed by time step
        # - a list of actions, indexed by time step
        # - a list of rewards, indexed by time step
        # TODO: Implement this method.
        states = []
        actions = []
        rewards = []

        state = env.reset()
        terminal = False
        cur_state = state

        while not terminal:
            # render if set to True
            if render:
                env.render()
            action_softmax = model.predict(np.array([cur_state]))
            action = action_softmax.argmax()
            next_state, reward, terminal, d = env.step(action)

            # add to dataset
            states.append(cur_state)
            actions.append(action)
            rewards.append(reward)

            # update cur_state for next iter
            cur_state = next_state

        return states, actions, rewards

    def run_episodes(self, env, num_episodes=1, render=False):
        # Runs model in an env for num_episodes

        data = {'states': [], 'actions': [], 'rewards': []}

        for episode in range(num_episodes):
            states, actions, rewards = self.run_expert(env, render)
            data['states'].extend(states)
            data['actions'].extend(actions)
            data['rewards'].extend(rewards)

        pickle.dump(data, open('data/data_{}_episodes.p'.format(num_episodes),'wb'))
    
    def train(self, env, X, y, num_episodes=100, num_epochs=50, render=False):
        # Trains the model on training data generated by the expert policy.
        # Args:
        # - env: The environment to run the expert policy on. 
        # - num_episodes: # episodes to be generated by the expert.
        # - num_epochs: # epochs to train on the data generated by the expert.
        # - render: Whether to render the environment.
        # Returns the final loss and accuracy.
        # TODO: Implement this method. It may be helpful to call the class
        #       method run_expert() to generate training data.
        loss = 0
        acc = 0

        history = self.model.fit(X,y,epochs=num_epochs,verbose=0)
        self._save_model_weights('model/{}_episodes.h5'.format(num_episodes))
        loss = history.history['loss']
        acc = history.history['acc']
        return loss, acc

    def run_policy(self, env, model_type, num_episodes=50, render=False):
        # Runs policy of model_type in env for num_episodes
        # and returns mean and std over num_episodes
        tot_rewards = []
        if model_type == 'expert':
            for episode in range(num_episodes):
                _, actions, rewards = self.run_expert(env, render)
                # print('Episode : ' + str(episode) + ' Actions : ' + str(actions))
                tot_rewards.append(sum(rewards))
        else:
            for episode in range(num_episodes):
                _, actions, rewards = self.run_model(env, render)

                tot_rewards.append(sum(rewards))
        tot_rewards = np.array(tot_rewards)
        # print('Total rewards : ', tot_rewards)
        return np.mean(tot_rewards), np.std(tot_rewards)


def parse_arguments():
    # Command-line flags are defined here.
    parser = argparse.ArgumentParser()
    parser.add_argument('--model-config-path', dest='model_config_path',
                        type=str, default='LunarLander-v2-config.json',
                        help="Path to the model config file.")
    parser.add_argument('--expert-pickles-path', dest='expert_weights_path',
                        type=str, default='LunarLander-v2-weights.h5',
                        help="Path to the expert pickles file.")

    # https://stackoverflow.com/questions/15008758/parsing-boolean-values-with-argparse
    parser_group = parser.add_mutually_exclusive_group(required=False)
    parser_group.add_argument('--render', dest='render',
                              action='store_true',
                              help="Whether to render the environment.")
    parser_group.add_argument('--no-render', dest='render',
                              action='store_false',
                              help="Whether to render the environment.")
    parser.set_defaults(render=False)

    return parser.parse_args()


def createDirectories(l):
    for l in l:
        if not os.path.exists(l):
            os.makedirs(l)


def main(args):
    # Parse command-line arguments.
    args = parse_arguments()
    model_config_path = args.model_config_path
    expert_weights_path = args.expert_weights_path
    render = args.render
    
    dirs = [
        "data",
        "model"
    ]

    createDirectories(dirs)

    # Create the environment.
    env = gym.make('LunarLander-v2')
    
    # TODO: Train cloned models using imitation learning, and record their
    #       performance.
    im_agent = Imitation(model_config_path, expert_weights_path)

    for episode in [1,10,50,100]:
        im_agent.run_episodes(env, num_episodes=episode)

    for episode in [1,10,50,100]:
        data = pickle.load(open('data/data_{}_episodes.p'.format(episode),'rb'))
        X = data['states']
        y = data['actions']
        y = to_categorical(y,num_classes=env.action_space.n)
        loss, acc = im_agent.train(env,np.array(X),y,num_episodes=episode,num_epochs=50,render=False)
        print('Training accuracy after training on {} episode dataset : {}'.format(episode,acc[-1]))


    mean_r, std_r = im_agent.run_policy(env, 'expert', num_episodes=50, render=False)
    print('For expert, mean reward : {} and std dev : {}'.format(mean_r, std_r))

    for episode in [1,10,50,100]:
        im_agent.load_model_weights('model/{}_episodes.h5'.format(episode))
        mean_r, std_r = im_agent.run_policy(env,'model',num_episodes=50,render=False)
        print('For model trained on {} episodes, mean reward : {} and std dev : {}'.format(episode,mean_r,std_r))









if __name__ == '__main__':
  main(sys.argv)
